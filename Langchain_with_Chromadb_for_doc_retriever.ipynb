{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langchain doc retriever with ChromaDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get the OpenAI key from .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "# Import the function load_dotenv from the dotenv package.\n",
    "\n",
    "import os\n",
    "# Import the os module for interacting with the operating system.\n",
    "\n",
    "load_dotenv()\n",
    "# Load environment variables from a .env file.\n",
    "\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "# Get the value of the 'OPENAI_API_KEY' environment variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import TextLoader, DirectoryLoader\n",
    "from langchain.document_loaders import TextLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and process the text files\n",
    "loader = TextLoader('papers.txt')\n",
    "\n",
    "papers = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# splitting the text into\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "texts = text_splitter.split_documents(papers) \n",
    "# every chunk has 1000 chracters\n",
    "# below we see the features of OpenAI model = text-embedding-ada-002 and chunk_size=1000\n",
    "# chunk_overlap iki chuns arasi ortak karakter sayisi\n",
    "len(texts) # We have 114 chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed and store the text\n",
    "# Supplying a persist directory will store the embeddings on disk\n",
    "persist_directory = 'db'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = OpenAIEmbeddings()   \n",
    "# OpenAI embeddings modelini kullaniyoruz. onun kendine ait embeddings kümesi var ve tokenizer\n",
    "# bununla PDF ler ele aliniyor ve anlamlari ele aliniyor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = Chroma.from_documents(documents = texts, # bizim PDF lerin toplami\n",
    "                                 embedding = embedding, \n",
    "                                 persist_directory = persist_directory)\n",
    "                                 # owerwrite parametreside var. True ise m,evcut verilerin üstüne yazar, False ise yeni bir database olusturur\n",
    "\n",
    "# burda texts embeddings ile vectörlere cevrilir ve Chroma db e kaydedilir\n",
    "# ve calistirinca bir 'db' adinda bir database olusturulur ve icine kaydedilir\n",
    "# tüm belgelerimizi kodladi yani Openai embeddinginden gecti.database'e kaydedilme burda yapiliyor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# persiste the db to disk\n",
    "vectordb.persist()  # bu vector database diske kaydeder. yani datalari bir sürü diske kaydeder. database in boyutunu belirler ve icindekileri diske yazar\n",
    "vectordb = None # yukarida diske kaydedildigi icin bu degiskenin artik kullanilmayacagini gösterir.\n",
    "\n",
    "# vector database size'ini belirlemek icin vector database 'collection' ve documents özelliklerini kullanir\n",
    "# documents bizim kullandigimiz degisken icinde tüm textin oldugu\n",
    "\n",
    "# bunlari kaydedersekiyi olur. cünkü kaydetmezsek bu uygulamayi her actigimizda tekrardan Openai embeddingi yapmak gerekecek\n",
    "# kaydedersek ordan direkt kullanilir. \n",
    "# aslinda bu kaydetme islemini bir kere yazariz ve owerwrite parametresini ona göre ayarlayabiliriz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can load the persisted database from disk and use it as normal\n",
    "vectordb = Chroma(persist_directory = persist_directory,\n",
    "                  embedding_function = embedding)\n",
    "\n",
    "# daha önce diske kaydettigimiz Chromadb yi tekrardan kullanmak icin yapiyoruz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### make a retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# burda aslinda byte yapmak istedigi sey . ilgili sorunun dosyasini buluyor.\n",
    "# hem cevabin oldugu metni veriyor ve en altta dosyanin adinida veriyor\n",
    "# burda metinler chunks lar halinde geliyor. her 1000 charakter 1 chunks oluyor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Title: Llama 2: Open Foundation and Fine-Tuned Chat Models\\nSummary:   In this work, we develop and release Llama 2, a collection of pretrained and\\nfine-tuned large language models (LLMs) ranging in scale from 7 billion to 70\\nbillion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for\\ndialogue use cases. Our models outperform open-source chat models on most\\nbenchmarks we tested, and based on our human evaluations for helpfulness and\\nsafety, may be a suitable substitute for closed-source models. We provide a\\ndetailed description of our approach to fine-tuning and safety improvements of\\nLlama 2-Chat in order to enable the community to build on our work and\\ncontribute to the responsible development of LLMs.', metadata={'source': 'papers.txt'}),\n",
       " Document(page_content='Title: Financial News Analytics Using Fine-Tuned Llama 2 GPT Model\\nSummary:   The paper considers the possibility to fine-tune Llama 2 GPT large language\\nmodel (LLM) for the multitask analysis of financial news. For fine-tuning, the\\nPEFT/LoRA based approach was used. In the study, the model was fine-tuned for\\nthe following tasks: analysing a text from financial market perspectives,\\nhighlighting main points of a text, summarizing a text and extracting named\\nentities with appropriate sentiments. The obtained results show that the\\nfine-tuned Llama 2 model can perform a multitask financial news analysis with a\\nspecified structure of response, part of response can be a structured text and\\nanother part of data can have JSON format for further processing. Extracted\\nsentiments for named entities can be considered as predictive features in\\nsupervised machine learning models with quantitative target variables.', metadata={'source': 'papers.txt'}),\n",
       " Document(page_content='safety training of Llama 2-Chat models of sizes 7B, 13B, and 70B. Specifically,\\nour fine-tuning technique significantly reduces the rate at which the model\\nrefuses to follow harmful instructions. We achieve a refusal rate below 1% for\\nour 70B Llama 2-Chat model on two refusal benchmarks. Our fine-tuning method\\nretains general performance, which we validate by comparing our fine-tuned\\nmodels against Llama 2-Chat across two benchmarks. Additionally, we present a\\nselection of harmful outputs produced by our models. While there is\\nconsiderable uncertainty about the scope of risks from current models, it is\\nlikely that future models will have significantly more dangerous capabilities,\\nincluding the ability to hack into critical infrastructure, create dangerous\\nbio-weapons, or autonomously replicate and adapt to new environments. We show\\nthat subversive fine-tuning is practical and effective, and hence argue that', metadata={'source': 'papers.txt'}),\n",
       " Document(page_content=\"existing benchmarks such as the OpenAI Moderation Evaluation dataset and\\nToxicChat, where its performance matches or exceeds that of currently available\\ncontent moderation tools. Llama Guard functions as a language model, carrying\\nout multi-class classification and generating binary decision scores.\\nFurthermore, the instruction fine-tuning of Llama Guard allows for the\\ncustomization of tasks and the adaptation of output formats. This feature\\nenhances the model's capabilities, such as enabling the adjustment of taxonomy\\ncategories to align with specific use cases, and facilitating zero-shot or\\nfew-shot prompting with diverse taxonomies at the input. We are making Llama\\nGuard model weights available and we encourage researchers to further develop\\nand adapt them to meet the evolving needs of the community for AI safety.\", metadata={'source': 'papers.txt'})]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = vectordb.as_retriever()   # bu database'i bir arama motoruna cevirir. ve metin aramsi yapmak icin kullanilir\n",
    "# bir üsttekiyle retriever baglantili hale getiriliyor. baglantili haliyle bir asagida yeni bir function icinde kullanilabiliyor\n",
    "docs = retriever.get_relevant_documents('For which tasks has Llama-2 already been used successfully? What are promising areas of application for Llama-2?') # bu bir liste döndürür. burda bir degisken adi yazip, yularida bu degisken adinin oldugu yere soruyu yazabiliriz, daha okunakli olur\n",
    "docs\n",
    "\n",
    "# bu retriever asagida chain olusturulken kullanacagiz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)\n",
    "# burdanda 4 tane chunks oldugunu görebiliyorum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make a Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the chain to answer questions\n",
    "# burda bir tanimlama yaoacagiz ve asagida sorgu yaparken kullanacagiz\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=OpenAI(),\n",
    "                                        chain_type='stuff',\n",
    "                                        retriever=retriever,\n",
    "                                        return_source_documents = True)\n",
    "\n",
    "# qa_chain = bunu yazdirirsak icinde Retrival ile bilgileri yazdiriyor\n",
    "# openAi killandigimizi gösteriyor ve open_key gibi biligler veriyor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'For which tasks has Llama-2 already been used successfully? What are promising areas of application for Llama-2?',\n",
       " 'result': ' Llama 2 has been successfully used for dialogue and financial news analysis tasks. Promising areas of application for Llama 2 include content moderation, language classification, and AI safety.',\n",
       " 'source_documents': [Document(page_content='Title: Llama 2: Open Foundation and Fine-Tuned Chat Models\\nSummary:   In this work, we develop and release Llama 2, a collection of pretrained and\\nfine-tuned large language models (LLMs) ranging in scale from 7 billion to 70\\nbillion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for\\ndialogue use cases. Our models outperform open-source chat models on most\\nbenchmarks we tested, and based on our human evaluations for helpfulness and\\nsafety, may be a suitable substitute for closed-source models. We provide a\\ndetailed description of our approach to fine-tuning and safety improvements of\\nLlama 2-Chat in order to enable the community to build on our work and\\ncontribute to the responsible development of LLMs.', metadata={'source': 'papers.txt'}),\n",
       "  Document(page_content='Title: Financial News Analytics Using Fine-Tuned Llama 2 GPT Model\\nSummary:   The paper considers the possibility to fine-tune Llama 2 GPT large language\\nmodel (LLM) for the multitask analysis of financial news. For fine-tuning, the\\nPEFT/LoRA based approach was used. In the study, the model was fine-tuned for\\nthe following tasks: analysing a text from financial market perspectives,\\nhighlighting main points of a text, summarizing a text and extracting named\\nentities with appropriate sentiments. The obtained results show that the\\nfine-tuned Llama 2 model can perform a multitask financial news analysis with a\\nspecified structure of response, part of response can be a structured text and\\nanother part of data can have JSON format for further processing. Extracted\\nsentiments for named entities can be considered as predictive features in\\nsupervised machine learning models with quantitative target variables.', metadata={'source': 'papers.txt'}),\n",
       "  Document(page_content='safety training of Llama 2-Chat models of sizes 7B, 13B, and 70B. Specifically,\\nour fine-tuning technique significantly reduces the rate at which the model\\nrefuses to follow harmful instructions. We achieve a refusal rate below 1% for\\nour 70B Llama 2-Chat model on two refusal benchmarks. Our fine-tuning method\\nretains general performance, which we validate by comparing our fine-tuned\\nmodels against Llama 2-Chat across two benchmarks. Additionally, we present a\\nselection of harmful outputs produced by our models. While there is\\nconsiderable uncertainty about the scope of risks from current models, it is\\nlikely that future models will have significantly more dangerous capabilities,\\nincluding the ability to hack into critical infrastructure, create dangerous\\nbio-weapons, or autonomously replicate and adapt to new environments. We show\\nthat subversive fine-tuning is practical and effective, and hence argue that', metadata={'source': 'papers.txt'}),\n",
       "  Document(page_content=\"existing benchmarks such as the OpenAI Moderation Evaluation dataset and\\nToxicChat, where its performance matches or exceeds that of currently available\\ncontent moderation tools. Llama Guard functions as a language model, carrying\\nout multi-class classification and generating binary decision scores.\\nFurthermore, the instruction fine-tuning of Llama Guard allows for the\\ncustomization of tasks and the adaptation of output formats. This feature\\nenhances the model's capabilities, such as enabling the adjustment of taxonomy\\ncategories to align with specific use cases, and facilitating zero-shot or\\nfew-shot prompting with diverse taxonomies at the input. We are making Llama\\nGuard model weights available and we encourage researchers to further develop\\nand adapt them to meet the evolving needs of the community for AI safety.\", metadata={'source': 'papers.txt'})]}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = 'For which tasks has Llama-2 already been used successfully? What are promising areas of application for Llama-2?'\n",
    "\n",
    "qa_chain(query)\n",
    "# burda gelen cevabin icinde aslinda nelerin oldugunu görebiliyoruz\n",
    "# query\n",
    "# result\n",
    "# source_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_llm_response(llm_response):\n",
    "    print(llm_response['result'])\n",
    "    print('\\n\\nSources:')\n",
    "\n",
    "    if llm_response['source_documents']:\n",
    "        source = llm_response['source_documents'][0]\n",
    "        print(source.metadata['source'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Llama-2 has been successfully used for fine-tuning LLMs for dialogue use cases, financial news analytics, and safety training. Promising areas of application for Llama-2 include financial news analysis, content moderation, and AI safety.\n",
      "\n",
      "\n",
      "Sources:\n",
      "papers.txt\n"
     ]
    }
   ],
   "source": [
    "query = 'For which tasks has Llama-2 already been used successfully? What are promising areas of application for Llama-2?'\n",
    "llm_response = qa_chain(query)\n",
    "process_llm_response(llm_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Five domain-specific LLMs that have been created by fine-tuning Llama-2 are Lawyer LLaMA, Llama 2-Chat, Llama 2 GPT, Llama 2 model for financial news analysis, and LLaMA for specific writing tasks.\n",
      "\n",
      "\n",
      "Sources:\n",
      "papers.txt\n"
     ]
    }
   ],
   "source": [
    "query = 'Name at least 5 domain-specific LLMs that have been created by fine-tuning Llama-2.'\n",
    "llm_response = qa_chain(query)\n",
    "process_llm_response(llm_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Based on the provided context, the Llama-2 model has a range of scales from 7 billion to 70 billion parameters. The fine-tuned Llama 2-Chat models are optimized for dialogue use cases and outperform open-source chat models on most benchmarks. The Code Llama family of models, based on Llama 2, also have different variations such as foundation models, Python specializations, and instruction-following models with 7B, 13B, and 34B parameters each. These models are trained on sequences of 16k tokens and show improvements on inputs with up to 100k tokens. The 7B and 13B Code Llama and Code Llama - Instruct variants also support infilling based on surrounding content. The models achieved state-of-the-art performance among open models on several code benchmarks, with scores of up to 53% and 55% on HumanEval and MBPP, respectively. The required memory and computing capacity for these models may vary depending on the specific variant and task at hand. The available quantizations for these models may also vary.\n",
      "\n",
      "\n",
      "Sources:\n",
      "papers.txt\n"
     ]
    }
   ],
   "source": [
    "query = 'What can you find out about the model structure of Llama-2 (required memory, required computing capacity, number of parameters, available quantizations)?'\n",
    "llm_response = qa_chain(query)\n",
    "process_llm_response(llm_response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.18 ('algonaut_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c5324f1c1d42ea829d34197da467997651adb4f8c0266714a6cf2f180aa110b3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
