{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langchain doc retriever with ChromaDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get the OpenAI key from .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "# Import the function load_dotenv from the dotenv package.\n",
    "\n",
    "import os\n",
    "# Import the os module for interacting with the operating system.\n",
    "\n",
    "load_dotenv()\n",
    "# Load environment variables from a .env file.\n",
    "\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "# Get the value of the 'OPENAI_API_KEY' environment variable.\n",
    "# I have a .gitignore file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import TextLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and process the text files\n",
    "# The 'papers' file was created in the 'get_the_text_from_arxiv.ipynb' file.\"\n",
    "loader = TextLoader('papers.txt')\n",
    "\n",
    "papers = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# splitting the text into\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "papers = text_splitter.split_documents(papers) \n",
    "\n",
    "# every chunk has 1000 chracters\n",
    "# below we see the features of OpenAI model = text-embedding-ada-002 and chunk_size=1000\n",
    "# we can not hit the size chunk of OpenAI Embeddings\n",
    "\n",
    "# This variable starts with the first 200 characters of a chunk and the last 200 characters of the previous chunk.\n",
    "# It's not necessary, but it positively affects the results.\n",
    "\n",
    "len(papers) # We have 114 chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed and store the text\n",
    "# Supplying a persist directory will store the embeddings on disk\n",
    "persist_directory = 'db'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = OpenAIEmbeddings()   \n",
    "\n",
    "# We are using the OpenAI embeddings model. \n",
    "# The 'papers' text is being converted into vectors with OpenAI embeddings.\n",
    "# print(embeddings) When we run the command 'print(embeddings)', we can see the features of the model.\n",
    "# I didn't run this print(embeddings) because the 'openai_api_key' appears inside when it is run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = Chroma.from_documents(documents = papers, # our all papers from ArXiv\n",
    "                                 embedding = embedding, \n",
    "                                 persist_directory = persist_directory)\n",
    "                                 \n",
    "\n",
    "# Here, the 'papers' text is converted into vectors using OpenAI embeddings \n",
    "# and saved in Chromadb. When the code is run,\n",
    "# a database named 'db' is created and the data is stored inside it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# persiste the db to disk\n",
    "vectordb.persist()  # The converted vectors are saved to disk.\n",
    "vectordb = None     # Since we have saved it to the disk, let's invalidate the above parameter.\n",
    "\n",
    "# The purpose of saving to disk is so that the vectors can be retrieved \n",
    "# from the saved location each time the program runs.\n",
    "# If we don't save them, the entire text will need to be converted \n",
    "# into vectors again every time it runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can load the persisted database from disk and use it as normal\n",
    "vectordb = Chroma(persist_directory = persist_directory,\n",
    "                  embedding_function = embedding)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### make a retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectordb.as_retriever()\n",
    "\n",
    "#This 'vector database' is transformed into a search engine \n",
    "# and used for text searches. \n",
    "# It is connected to the retriever in the code above and can be used in a function below.\n",
    "#  We will use this 'retriever' when creating a 'chain' below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make a Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the chain to answer questions\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=OpenAI(),\n",
    "                                        chain_type='stuff',              # The type of retrieval QA chain to use\n",
    "                                        retriever=retriever,             # The retriever to use to retrieve documents\n",
    "                                        return_source_documents = True)  # Whether or not to return the source documents for each answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'For which tasks has Llama-2 already been used successfully? What are promising areas of application for Llama-2?',\n",
       " 'result': ' Llama-2 has been successfully used for fine-tuning large language models for dialogue use cases and financial news analysis. Promising areas of application for Llama-2 include language tasks such as text analysis, summarization, and named entity extraction with sentiment analysis, as well as other use cases that require large language models such as predictive features in supervised machine learning models.',\n",
       " 'source_documents': [Document(page_content='Title: Llama 2: Open Foundation and Fine-Tuned Chat Models\\nSummary:   In this work, we develop and release Llama 2, a collection of pretrained and\\nfine-tuned large language models (LLMs) ranging in scale from 7 billion to 70\\nbillion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for\\ndialogue use cases. Our models outperform open-source chat models on most\\nbenchmarks we tested, and based on our human evaluations for helpfulness and\\nsafety, may be a suitable substitute for closed-source models. We provide a\\ndetailed description of our approach to fine-tuning and safety improvements of\\nLlama 2-Chat in order to enable the community to build on our work and\\ncontribute to the responsible development of LLMs.', metadata={'source': 'papers.txt'}),\n",
       "  Document(page_content='Title: Llama 2: Open Foundation and Fine-Tuned Chat Models\\nSummary:   In this work, we develop and release Llama 2, a collection of pretrained and\\nfine-tuned large language models (LLMs) ranging in scale from 7 billion to 70\\nbillion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for\\ndialogue use cases. Our models outperform open-source chat models on most\\nbenchmarks we tested, and based on our human evaluations for helpfulness and\\nsafety, may be a suitable substitute for closed-source models. We provide a\\ndetailed description of our approach to fine-tuning and safety improvements of\\nLlama 2-Chat in order to enable the community to build on our work and\\ncontribute to the responsible development of LLMs.', metadata={'source': 'papers.txt'}),\n",
       "  Document(page_content='Title: Llama 2: Open Foundation and Fine-Tuned Chat Models\\nSummary:   In this work, we develop and release Llama 2, a collection of pretrained and\\nfine-tuned large language models (LLMs) ranging in scale from 7 billion to 70\\nbillion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for\\ndialogue use cases. Our models outperform open-source chat models on most\\nbenchmarks we tested, and based on our human evaluations for helpfulness and\\nsafety, may be a suitable substitute for closed-source models. We provide a\\ndetailed description of our approach to fine-tuning and safety improvements of\\nLlama 2-Chat in order to enable the community to build on our work and\\ncontribute to the responsible development of LLMs.', metadata={'source': 'papers.txt'}),\n",
       "  Document(page_content='Title: Financial News Analytics Using Fine-Tuned Llama 2 GPT Model\\nSummary:   The paper considers the possibility to fine-tune Llama 2 GPT large language\\nmodel (LLM) for the multitask analysis of financial news. For fine-tuning, the\\nPEFT/LoRA based approach was used. In the study, the model was fine-tuned for\\nthe following tasks: analysing a text from financial market perspectives,\\nhighlighting main points of a text, summarizing a text and extracting named\\nentities with appropriate sentiments. The obtained results show that the\\nfine-tuned Llama 2 model can perform a multitask financial news analysis with a\\nspecified structure of response, part of response can be a structured text and\\nanother part of data can have JSON format for further processing. Extracted\\nsentiments for named entities can be considered as predictive features in\\nsupervised machine learning models with quantitative target variables.', metadata={'source': 'papers.txt'})]}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = 'For which tasks has Llama-2 already been used successfully? What are promising areas of application for Llama-2?'\n",
    "\n",
    "qa_chain(query)\n",
    "\n",
    "# This is our response value.\n",
    "# It contains outputs for query, result, and source_documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_llm_response(llm_response):\n",
    "    print(llm_response['result'])\n",
    "    print('\\n\\nSources:')\n",
    "\n",
    "    if llm_response['source_documents']:\n",
    "        source = llm_response['source_documents'][0]\n",
    "        print(source.metadata['source'])\n",
    "\n",
    "# This function written to better see the outputs.\n",
    "# I will use this function for query output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Llama-2 has been successfully used for analyzing financial news from a market perspective, highlighting main points, summarizing text, and extracting named entities with appropriate sentiments. Some promising areas of application for Llama-2 include financial news analysis, multitask analysis, and potentially as a substitute for closed-source models in dialogue use cases.\n",
      "\n",
      "\n",
      "Sources:\n",
      "papers.txt\n"
     ]
    }
   ],
   "source": [
    "query = 'For which tasks has Llama-2 already been used successfully? What are promising areas of application for Llama-2?'\n",
    "llm_response = qa_chain(query)\n",
    "process_llm_response(llm_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I don't know.\n",
      "\n",
      "\n",
      "Sources:\n",
      "papers.txt\n"
     ]
    }
   ],
   "source": [
    "query = 'Name at least 5 domain-specific LLMs that have been created by fine-tuning Llama-2.'\n",
    "llm_response = qa_chain(query)\n",
    "process_llm_response(llm_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unfortunately, the summary does not provide enough information about the model structure of Llama-2 to answer this question. It mentions the scale of the models (ranging from 7 billion to 70 billion parameters) and that they are optimized for dialogue use cases, but it does not mention anything about required memory, computing capacity, or available quantizations.\n",
      "\n",
      "\n",
      "Sources:\n",
      "papers.txt\n"
     ]
    }
   ],
   "source": [
    "query = 'What can you find out about the model structure of Llama-2 (required memory, required computing capacity, number of parameters, available quantizations)?'\n",
    "llm_response = qa_chain(query)\n",
    "process_llm_response(llm_response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.18 ('algonaut_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c5324f1c1d42ea829d34197da467997651adb4f8c0266714a6cf2f180aa110b3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
